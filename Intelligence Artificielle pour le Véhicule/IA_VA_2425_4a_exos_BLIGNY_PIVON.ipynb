{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4. Classification par réseaux de neurones convolutifs</b>\n",
    "## <b>Partie 1 : Petits réseaux de neurones personnalisés</b>\n",
    "\n",
    "### <b>Exercice 4.1 : Réseau de neurones convolutifs avec données GTSRB (Keras)</b>\n",
    "\n",
    "Développer et tester un réseau de neurones convolutifs en Keras, permettant d'obtenir une <b>précision (accuracy) >=99,5%</b> aves les données GTSRB. Le modèle devra comporter <b>moins de 300000 paramètres</b>.\n",
    "\n",
    "On pourra reprendre dans un premier temps la structure générale du réseau de neurones déjà utilisé avec MNIST, mais il faudra ensuite l'adapter pour les images couleurs et pour améliorer sa précision.  \n",
    "On prendra une taille de 48x48 pour les images.\n",
    "\n",
    "On peut reprendre l'exemple complet de classification des données GTSRB avec un MLP.\n",
    "Pour l'amélioration de la précision, on pourra jouer sur :\n",
    "- le nombre de couches (convolutives et complètement connectées)\n",
    "- le nombre de feature-maps dans les couches convolutives\n",
    "- le pooling\n",
    "- le dropout\n",
    "- la taille des batchs\n",
    "- l'augmentation du dataset\n",
    "- la batch-normalisation\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de base\n",
    "input_shape = (48, 48, 3)  # Images 48x48 en couleur\n",
    "num_classes = 43  # Nombre de classes dans GTSRB\n",
    "data_path = '/home/jovyan/iadatasets/GTSRB/Final_Training/Images/'  # Chemin vers les données\n",
    "\n",
    "# Chargement et préparation des données\n",
    "def load_GTSRB():\n",
    "    print(\"Chargement des données...\")\n",
    "    data, labels = [], []\n",
    "    for i in range(num_classes):\n",
    "        image_path = os.path.join(data_path, format(i, '05d'))\n",
    "        print(f\"Chargement du répertoire {image_path}\")\n",
    "        for img_path in glob.glob(image_path + '/*.ppm'):\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.resize(image, (48, 48))  # Redimensionnement à 48x48\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "    print('Données chargées.')\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "X, y = load_GTSRB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Normalisation des données\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Création du modèle CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Couche convolutive 1\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Couche convolutive 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Couche d'aplatissement\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Couche de sortie\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(X_test, y_test)) # 30 epochs suffisent pour atteindre la précision\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Évaluation du modèle sur les données de test\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Précision sur l\\'ensemble de test : {score[1] * 100:.2f}%')\n",
    "\n",
    "# Affichage des courbes de précision\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Précision entraînement')\n",
    "plt.plot(history.history['val_accuracy'], label='Précision validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Perte entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Perte validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Commentaires</b>\n",
    "\n",
    "Nous avons ajouté 2 couches de convolution, possédant un nombre croissant de feature maps (64 puis 128), ce qui nous permet de ne pas dépasser 300k paramètres. Les couches sont suivies de Batch Normalization et de MaxPooling2D pour stabiliser l'apprentissage et réduire la taille des cartes de caractéristiques. Nous avons commencé l'apprentissage avec 50 epochs, ce qui nous permet d'atteindre largement l'objectif mais qui est trop long. Nous avons donc reéssayé avec 20 epochs, ce qui est suffisant pour atteindre la précision demandée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 4.2 : Réseau de neurones convolutifs avec données GTSRB (Pytorch)</b>\n",
    "\n",
    "Idem exercice 4.1 mais avec Pytorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 1/30, Perte: 1.3738, Précision: 61.90%\n",
      "Époque 2/30, Perte: 0.4533, Précision: 86.24%\n",
      "Époque 3/30, Perte: 0.2568, Précision: 91.95%\n",
      "Époque 4/30, Perte: 0.1975, Précision: 93.80%\n",
      "Époque 5/30, Perte: 0.1613, Précision: 94.88%\n",
      "Époque 6/30, Perte: 0.1500, Précision: 95.18%\n",
      "Époque 7/30, Perte: 0.1319, Précision: 95.79%\n",
      "Époque 8/30, Perte: 0.1217, Précision: 96.05%\n",
      "Époque 9/30, Perte: 0.1146, Précision: 96.27%\n",
      "Époque 10/30, Perte: 0.0993, Précision: 96.70%\n",
      "Époque 11/30, Perte: 0.0898, Précision: 97.27%\n",
      "Époque 12/30, Perte: 0.0923, Précision: 96.88%\n",
      "Époque 13/30, Perte: 0.0852, Précision: 97.29%\n",
      "Époque 14/30, Perte: 0.0857, Précision: 97.06%\n",
      "Époque 15/30, Perte: 0.0751, Précision: 97.47%\n",
      "Époque 16/30, Perte: 0.0742, Précision: 97.55%\n",
      "Époque 17/30, Perte: 0.0695, Précision: 97.65%\n",
      "Époque 18/30, Perte: 0.0608, Précision: 98.04%\n",
      "Époque 19/30, Perte: 0.0652, Précision: 97.95%\n",
      "Époque 20/30, Perte: 0.0618, Précision: 97.98%\n",
      "Époque 21/30, Perte: 0.0596, Précision: 97.98%\n",
      "Époque 22/30, Perte: 0.0600, Précision: 98.03%\n",
      "Époque 23/30, Perte: 0.0569, Précision: 98.14%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Paramètres de configuration\n",
    "input_shape = (48, 48, 3)  # Images 48x48 en couleur\n",
    "num_classes = 43  # Nombre de classes dans GTSRB\n",
    "data_path = '/home/jovyan/iadatasets/GTSRB/Final_Training/Images/'\n",
    "\n",
    "# Chargement des données et définition des transformations\n",
    "class GTSRBDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Chargement et préparation des données\n",
    "def load_GTSRB():\n",
    "    data, labels = [], []\n",
    "    for i in range(num_classes):\n",
    "        image_path = os.path.join(data_path, format(i, '05d'))\n",
    "        for img_path in glob.glob(image_path + '/*.ppm'):\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.resize(image, (48, 48))  # Redimensionnement à 48x48\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Transformations avec augmentation de données\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomRotation(15),  # Rotation\n",
    "    transforms.RandomHorizontalFlip(),  # Flip horizontal\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Chargement des données\n",
    "X, y = load_GTSRB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Création des jeux de données PyTorch\n",
    "train_dataset = GTSRBDataset(X_train, y_train, transform=transform)\n",
    "test_dataset = GTSRBDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "# Création des DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Création du modèle CNN\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel(num_classes=num_classes)\n",
    "\n",
    "# Critère et Optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Fonction d'entraînement avec suivi des pertes et précisions\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f'Époque {epoch+1}/{num_epochs}, Perte: {epoch_loss:.4f}, Précision: {epoch_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Fonction évaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Précision sur l\\'ensemble de test : {test_accuracy:.2f}%')\n",
    "    return test_accuracy\n",
    "\n",
    "# Entraînement du modèle\n",
    "train_losses, train_accuracies = train_model(model, train_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# Évaluation du modèle\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "# Affichage des courbes de précision\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label='Précision entraînement')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Perte entraînement')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <b>Commentaires</b>\n",
    "\n",
    "Nous avons adapté notre réseau de neurones en PyTorch. Nous avons d'abord préparé les données avec des transformations pour augmenter la diversité des images, puis avons entraîné notre modèle avec 30 époques en suivant les pertes et la précision à chaque étape. Ensuite, nous avons évalué la performance de notre modèle sur l'ensemble de test, en affichant les courbes de précision et de perte pour observer l'évolution de l'apprentissage et des résultats. Le modèle respecte bien la limite de 300 000 paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 4.3 : Chargement d'un modèle généré précédemment et utilisation en inférence</b>\n",
    "\n",
    "Appliquer le modèle en <b>inférence</b> sur au moins une image du dataset de test (il faudra pour cela au préalable convertir cette image au format tenseur de torch puis l'appliquer en entrée du modèle).  \n",
    "Mesurer la <b>durée</b> d'inférence (en l'appliquant sur la même image et pour un grand nombre d'itérations, et en affichant cette durée à chaque fois).\n",
    "Afficher sur l'image si la réponse est correcte ou non.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Transformation des images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Fonction pour charger et transformer une image pour l'inférence\n",
    "def transform_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (48, 48))  # Redimensionner en 48x48\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Conversion en RGB\n",
    "    image = Image.fromarray(image)  # Conversion en image PIL\n",
    "    image = transform(image).unsqueeze(0)  # Ajout de la dimension du batch\n",
    "    return image\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model_path = \"/home/jovyan/TP1/modele/modele.pth\"  # Chemin vers le modèle sauvegardé\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Déplacement du modèle sur le GPU si disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Charger une image de test\n",
    "test_image_path = '/home/jovyan/iadatasets/GTSRB/Final_Training/Images/00000/00000_00000.ppm'\n",
    "true_label = 0  # Classe réelle de l'image test\n",
    "image_tensor = transform_image(test_image_path)\n",
    "\n",
    "# Déplacement de l'image sur le GPU si disponible\n",
    "image_tensor = image_tensor.to(device)\n",
    "\n",
    "# Inférence sur plusieurs itérations\n",
    "num_iterations = 10\n",
    "durations = []\n",
    "correct_predictions = 0\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        output = model(image_tensor)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Enregistrer la durée d'inférence\n",
    "        durations.append(end_time - start_time)\n",
    "\n",
    "        # Vérifier si la prédiction est correcte\n",
    "        if predicted_class.item() == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "# Durée moyenne d'inférence\n",
    "average_inference_time = sum(durations) / num_iterations\n",
    "print(f\"Durée moyenne d'inférence : {average_inference_time:.4f} secondes\")\n",
    "print(f\"Prédictions correctes : {correct_predictions}/{num_iterations}\")\n",
    "\n",
    "# Résultat de la prédiction pour affichage\n",
    "result_text = \"Prédiction correcte !\" if predicted_class.item() == true_label else f\"Prédiction incorrecte. Classe prédite : {predicted_class.item()}\"\n",
    "\n",
    "# Afficher l'image avec le résultat\n",
    "image = cv2.imread(test_image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.title(f\"{result_text} (Classe réelle : {true_label})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Commentaires</b>\n",
    "\n",
    "Nous avons repris le modèle de l'exercice précédent que nous avons appliqué en inférence sur la première image du dataset."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
