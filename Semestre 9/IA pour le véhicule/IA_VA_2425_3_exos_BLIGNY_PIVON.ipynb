{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 3.1 : MLP avec données MNIST, avec librairie Scikit-Learn</b> \n",
    "\n",
    "Appliquer la fonction MLPClassifier de Scikiyt-Learn aux données MNIST, en essayer d'obtenir la meilleure précision possible tout en gardant une durée d'apprentissage raisonnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/tmp/ipykernel_64/4286784621.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X, y = mnist['data'], mnist['target'].astype(np.int)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 92.36%\n",
      "Durée d'entraînement: 20 itérations\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "#chargement des données\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target'].astype(np.int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#Initialisation du modèle MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=20, alpha=1e-5, solver='sgd', random_state=1)\n",
    "\n",
    "#entraînement du modèle\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Précision: {accuracy * 100:.2f}%')\n",
    "print(f'Durée d\\'entraînement: {mlp.n_iter_} itérations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### <b>Commentaires :</b>\n",
    "\n",
    "La précision est 92.36% avec un apprentissage de 20 itérations, ce qui nous parait raisonnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 3.2 : MLP avec données MNIST, avec librairie Keras</b>\n",
    "\n",
    "Essayer d'améliorer la précision du réseau de neurones (accuracy) utilisé sur les données MNIST dans le notebook principal, en limitant le nombre de paramètres du réseau de neurones. On peut jouer pour ça sur :\n",
    "\n",
    "- la structure du réseau de neurones (nombre de couches cachées, nombre de neurones dans les couches)\n",
    "- l'ajout de dropout\n",
    "- la normalisation des données\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:39:50.657390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 14:39:51.751546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-16 14:39:53.154148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7962 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe MIG 1g.10gb, pci bus id: 0000:c3:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:39:54.818185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-16 14:39:54.909310: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7cc9dc048960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-16 14:39:54.909378: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100 80GB PCIe MIG 1g.10gb, Compute Capability 8.0\n",
      "2024-11-16 14:39:54.917095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-16 14:39:54.949306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2024-11-16 14:39:55.122018: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 4s 4ms/step - loss: 0.3784 - accuracy: 0.8844 - val_loss: 0.1352 - val_accuracy: 0.9568\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1629 - accuracy: 0.9517 - val_loss: 0.0932 - val_accuracy: 0.9710\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1230 - accuracy: 0.9632 - val_loss: 0.0784 - val_accuracy: 0.9750\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1007 - accuracy: 0.9695 - val_loss: 0.0719 - val_accuracy: 0.9780\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0858 - accuracy: 0.9737 - val_loss: 0.0723 - val_accuracy: 0.9778\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0759 - accuracy: 0.9761 - val_loss: 0.0637 - val_accuracy: 0.9803\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.0694 - val_accuracy: 0.9776\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0605 - accuracy: 0.9805 - val_loss: 0.0680 - val_accuracy: 0.9804\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0560 - accuracy: 0.9821 - val_loss: 0.0628 - val_accuracy: 0.9812\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0520 - accuracy: 0.9830 - val_loss: 0.0606 - val_accuracy: 0.9816\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0475 - accuracy: 0.9847 - val_loss: 0.0668 - val_accuracy: 0.9796\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0441 - accuracy: 0.9853 - val_loss: 0.0612 - val_accuracy: 0.9830\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0428 - accuracy: 0.9858 - val_loss: 0.0597 - val_accuracy: 0.9825\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0398 - accuracy: 0.9876 - val_loss: 0.0682 - val_accuracy: 0.9809\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0378 - accuracy: 0.9877 - val_loss: 0.0599 - val_accuracy: 0.9830\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.0667 - val_accuracy: 0.9829\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0625 - val_accuracy: 0.9829\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.0628 - val_accuracy: 0.9838\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0319 - accuracy: 0.9893 - val_loss: 0.0637 - val_accuracy: 0.9828\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0311 - accuracy: 0.9897 - val_loss: 0.0603 - val_accuracy: 0.9831\n",
      "Précision sur l'ensemble de test : 98.31%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Chargement des données\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "#Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))  #Ajout de Dropout pour éviter l'overfitting\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax'))  # Couche de sortie avec 10 classes\n",
    "\n",
    "#Compilation du modèle\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# évaluation du modèle sur les données de test\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Précision sur l\\'ensemble de test : {score[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Commentaires :</b>\n",
    "\n",
    "Nous avons commencé par normaliser les données. Le réseau de neurones utilise donc 256 neurones dans la première couche et 128 neurones dans la deuxième couche. Nous avons ajouté un dropout de 30% (0.3) et nous obtenons une meilleur précision : 98.31%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 3.3 : MLP avec données MNIST, avec librairie Pytorch</b> \n",
    "\n",
    "Idem exercice que 3.2 mais en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformation des données\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Conversion des images en tenseurs\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) #normalisation pour moyenne nulle et variance de 1\n",
    "])\n",
    "\n",
    "#Chargement des données MNIST\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "#Définition des DataLoader pour l'entraînement et les tests\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "#Définition du modèle\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256) # Couche cachée avec 256 neurones\n",
    "        self.dropout1 = nn.Dropout(0.3) # Dropout de 30% pour éviter l'overfitting\n",
    "        self.fc2 = nn.Linear(256, 128) # Couche cachée avec 128 neurones\n",
    "        self.dropout2 = nn.Dropout(0.3) # Dropout de 30% pour la deuxième couche\n",
    "        self.fc3 = nn.Linear(128, 10) # Couche de sortie avec 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Commentaires :</b>\n",
    "\n",
    "Nous avons repris le modèle de l'exercice 3.2 (nombre de neurones et dropout). Nous avons normalisé les données et nous avons optimisé avec Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 3.4 : MLP avec données GTSRB, avec librairie Pytorch</b> \n",
    "\n",
    "Appliquer aux données GTSRB le modèle étudié ci-dessus avec les données MNIST.\n",
    "\n",
    "Par rapport à l'exemple avec les données MNIST, il faudra faire quelques adaptations : \n",
    "- les images GTSRB étant des images en couleurs, il faudra les convertir en niveaux de gris car le MLP ne gère par les canaux couleur des images\n",
    "- adapter le nombre de classes\n",
    "- adapter la dimension des images (on pourra prendre par exemple 48x48 ou 32x32)\n",
    "- dans la fonction de chargement des images, ajouter une conversion en float (ex : data = np.array(images, dtype='float32') où images est la liste des images)\n",
    "- séparer données de test et d'apprentissage par exemple à l'aide de la fonction \"train_test_split\" de Scikit-Learn\n",
    "- changer l'optimiseur utilisé pour l'apprentissage (ex. \"Adam\")\n",
    "- on peut également limiter le nombre d'images par classe (ex. 00), pour limiter les temps de traitement\n",
    "- ...\n",
    "\n",
    "L'objectif est d'atteindre une précision de <b>94%</b> avec un modèle possédant <b>moins de 500000 paramètres</b>.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/14366]\tLoss: 3.761223\n",
      "Epoch 1 [12800/14366]\tLoss: 2.294377\n",
      "\n",
      "Test set: Average loss: 0.0171, Accuracy: 1420/3592 (39.53%)\n",
      "\n",
      "Epoch 2 [0/14366]\tLoss: 2.108185\n",
      "Epoch 2 [12800/14366]\tLoss: 1.590467\n",
      "\n",
      "Test set: Average loss: 0.0102, Accuracy: 2140/3592 (59.58%)\n",
      "\n",
      "Epoch 3 [0/14366]\tLoss: 1.459478\n",
      "Epoch 3 [12800/14366]\tLoss: 1.193142\n",
      "\n",
      "Test set: Average loss: 0.0084, Accuracy: 2498/3592 (69.54%)\n",
      "\n",
      "Epoch 4 [0/14366]\tLoss: 1.259585\n",
      "Epoch 4 [12800/14366]\tLoss: 1.079097\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 2817/3592 (78.42%)\n",
      "\n",
      "Epoch 5 [0/14366]\tLoss: 0.732259\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Définition des classes pour GTSRB (43 classes)\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "# Dimensions des images après redimensionnement\n",
    "IMAGE_SIZE = 48  # Augmenté à 48x48 pour plus de précision\n",
    "\n",
    "# Classe pour charger le dataset GTSRB\n",
    "class GTSRBDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Conversion en image PIL\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Charger les données GTSRB\n",
    "def load_gtsrb_data(data_dir, limit_per_class=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Extensions d'image acceptées\n",
    "    valid_extensions = ('.ppm', '.jpg', '.jpeg', '.png')\n",
    "    \n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        class_dir = os.path.join(data_dir, f'{class_id:05d}')\n",
    "        files = os.listdir(class_dir)\n",
    "        \n",
    "        if limit_per_class:\n",
    "            files = files[:limit_per_class]\n",
    "        \n",
    "        for file_name in files:\n",
    "            if file_name.lower().endswith(valid_extensions):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                try:\n",
    "                    image = Image.open(file_path).convert('L')  # Convertir en niveaux de gris\n",
    "                    image = np.array(image.resize((IMAGE_SIZE, IMAGE_SIZE)), dtype='float32')\n",
    "                    images.append(image)\n",
    "                    labels.append(class_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fichier ignoré (non valide): {file_path}, Erreur: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Charger les données GTSRB\n",
    "data_dir = '/home/jovyan/iadatasets/GTSRB/Final_Training/Images/'\n",
    "images, labels = load_gtsrb_data(data_dir, limit_per_class=500)    \n",
    "\n",
    "# Normalisation des données\n",
    "images = images / 255.0  # Normalisation entre 0 et 1\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transformation des images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalisation autour de 0\n",
    "])\n",
    "\n",
    "# Création des datasets et loaders\n",
    "train_dataset = GTSRBDataset(X_train, y_train, transform=transform)\n",
    "test_dataset = GTSRBDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Définition du modèle MLP pour GTSRB\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMAGE_SIZE * IMAGE_SIZE, 1024)  # Augmentation à 1024 neurones\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(1024, 512)  # Deuxième couche avec 512 neurones\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(512, 256)  # Troisième couche avec 256 neurones\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(256, NUM_CLASSES)  # Couche de sortie\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IMAGE_SIZE * IMAGE_SIZE)  # Aplatir les images\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialisation du modèle, de la fonction de coût et de l'optimiseur\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement du modèle\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}')        \n",
    "            \n",
    "# Test du modèle\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "# Entraînement et test du modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 97\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Commentaires :</b>\n",
    "\n",
    "Notre code utilise un MLP avec PyTorch pour classifier les images du dataset GTSRB après les avoir converties en niveaux de gris et redimensionnées. Cependant, nous n'avons pas atteint l'objectif de 94% de précision avec moins de 500 000 paramètres (nous avons environ 93% avec 100 epochs)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
